{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are Recurrent Neural Networks\n",
    "\n",
    "\n",
    "The idea behind RNNs is to make use of sequential information. In a traditional neural network we assume that all inputs (and outputs) are independent of each other. But for many tasks that’s a very bad idea. If you want to predict the next word in a sentence you better know which words came before it. RNNs are called recurrent because they perform the same task for every element of a sequence, with the output being depended on the previous computations. Another way to think about RNNs is that they have a “memory” which captures information about what has been calculated so far. In theory RNNs can make use of information in arbitrarily long sequences, but in practice they are limited to looking back only a few steps (more on this later). Here is what a typical RNN looks like:\n",
    "\n",
    "![](http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg)\n",
    "\n",
    "The above diagram shows a RNN being unrolled (or unfolded) into a full network. By unrolling we simply mean that we write out the network for the complete sequence. For example, if the sequence we care about is a sentence of 5 words, the network would be unrolled into a 5-layer neural network, one layer for each word. The formulas that govern the computation happening in a RNN are as follows:\n",
    "\n",
    "   * x_t is the input at time step t. For example, x_1 could be a one-hot vector corresponding to the second word of a sentence.\n",
    "   * s_t is the hidden state at time step t. It’s the “memory” of the network. s_t is calculated based on the previous hidden state and the input at the current step: s_t=f(Ux_t + Ws_{t-1}). The function f usually is a nonlinearity such as tanh or ReLU.  s_{-1}, which is required to calculate the first hidden state, is typically initialized to all zeroes.\n",
    "   * o_t is the output at step t. For example, if we wanted to predict the next word in a sentence it would be a vector of probabilities across our vocabulary. o_t = \\mathrm{softmax}(Vs_t).\n",
    "\n",
    "There are a few things to note here:\n",
    "\n",
    "   * You can think of the hidden state s_t as the memory of the network. s_t captures information about what happened in all the previous time steps. The output at step o_t is calculated solely based on the memory at time t. As briefly mentioned above, it’s a bit more complicated  in practice because s_t typically can’t capture information from too many time steps ago.\n",
    "   * Unlike a traditional deep neural network, which uses different parameters at each layer, a RNN shares the same parameters (U, V, W above) across all steps. This reflects the fact that we are performing the same task at each step, just with different inputs. This greatly reduces the total number of parameters we need to learn.\n",
    "\n",
    "   * The above diagram has outputs at each time step, but depending on the task this may not be necessary. For example, when predicting the sentiment of a sentence we may only care about the final output, not the sentiment after each word. Similarly, we may not need inputs at each time step. The main feature of an RNN is its hidden state, which captures some information about a sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "We will build a character-wise RNN trained on Anna Karenina. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "<img src=\"http://karpathy.github.io/assets/rnn/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use. Here I'm creating a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3': 0,\n",
       " '*': 1,\n",
       " 't': 2,\n",
       " 'w': 3,\n",
       " 'V': 4,\n",
       " 'f': 5,\n",
       " ',': 6,\n",
       " '4': 7,\n",
       " 'K': 8,\n",
       " '_': 9,\n",
       " '8': 10,\n",
       " 'T': 11,\n",
       " 'q': 12,\n",
       " 'A': 13,\n",
       " 'Q': 14,\n",
       " '`': 15,\n",
       " 'I': 16,\n",
       " 'J': 17,\n",
       " 'z': 18,\n",
       " 's': 19,\n",
       " 'W': 20,\n",
       " ' ': 21,\n",
       " 'e': 22,\n",
       " 'M': 23,\n",
       " 'F': 24,\n",
       " 'i': 25,\n",
       " 'h': 26,\n",
       " 'o': 27,\n",
       " 'P': 28,\n",
       " 'U': 29,\n",
       " '-': 30,\n",
       " '\"': 31,\n",
       " 'B': 32,\n",
       " '6': 33,\n",
       " 'G': 34,\n",
       " 'O': 35,\n",
       " '%': 36,\n",
       " \"'\": 37,\n",
       " '7': 38,\n",
       " 'v': 39,\n",
       " '0': 40,\n",
       " 'N': 41,\n",
       " ')': 42,\n",
       " 'E': 43,\n",
       " 'L': 44,\n",
       " 'j': 45,\n",
       " 'C': 46,\n",
       " 'S': 47,\n",
       " '\\n': 48,\n",
       " '!': 49,\n",
       " 'd': 50,\n",
       " 'a': 51,\n",
       " 'D': 52,\n",
       " 'g': 53,\n",
       " 'x': 54,\n",
       " ';': 55,\n",
       " '2': 56,\n",
       " 'H': 57,\n",
       " ':': 58,\n",
       " 'y': 59,\n",
       " '.': 60,\n",
       " 'k': 61,\n",
       " 'l': 62,\n",
       " 'Z': 63,\n",
       " '9': 64,\n",
       " '5': 65,\n",
       " 'c': 66,\n",
       " 'Y': 67,\n",
       " '$': 68,\n",
       " 'r': 69,\n",
       " '1': 70,\n",
       " 'R': 71,\n",
       " 'n': 72,\n",
       " 'X': 73,\n",
       " '@': 74,\n",
       " 'b': 75,\n",
       " 'm': 76,\n",
       " 'p': 77,\n",
       " 'u': 78,\n",
       " '/': 79,\n",
       " '(': 80,\n",
       " '&': 81,\n",
       " '?': 82}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([46, 26, 51, ..., 19, 60, 48])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([46, 26, 51, ..., 19, 60, 48])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3': 0,\n",
       " '*': 1,\n",
       " 't': 2,\n",
       " 'w': 3,\n",
       " 'V': 4,\n",
       " 'f': 5,\n",
       " ',': 6,\n",
       " '4': 7,\n",
       " 'K': 8,\n",
       " '_': 9,\n",
       " '8': 10,\n",
       " 'T': 11,\n",
       " 'q': 12,\n",
       " 'A': 13,\n",
       " 'Q': 14,\n",
       " '`': 15,\n",
       " 'I': 16,\n",
       " 'J': 17,\n",
       " 'z': 18,\n",
       " 's': 19,\n",
       " 'W': 20,\n",
       " ' ': 21,\n",
       " 'e': 22,\n",
       " 'M': 23,\n",
       " 'F': 24,\n",
       " 'i': 25,\n",
       " 'h': 26,\n",
       " 'o': 27,\n",
       " 'P': 28,\n",
       " 'U': 29,\n",
       " '-': 30,\n",
       " '\"': 31,\n",
       " 'B': 32,\n",
       " '6': 33,\n",
       " 'G': 34,\n",
       " 'O': 35,\n",
       " '%': 36,\n",
       " \"'\": 37,\n",
       " '7': 38,\n",
       " 'v': 39,\n",
       " '0': 40,\n",
       " 'N': 41,\n",
       " ')': 42,\n",
       " 'E': 43,\n",
       " 'L': 44,\n",
       " 'j': 45,\n",
       " 'C': 46,\n",
       " 'S': 47,\n",
       " '\\n': 48,\n",
       " '!': 49,\n",
       " 'd': 50,\n",
       " 'a': 51,\n",
       " 'D': 52,\n",
       " 'g': 53,\n",
       " 'x': 54,\n",
       " ';': 55,\n",
       " '2': 56,\n",
       " 'H': 57,\n",
       " ':': 58,\n",
       " 'y': 59,\n",
       " '.': 60,\n",
       " 'k': 61,\n",
       " 'l': 62,\n",
       " 'Z': 63,\n",
       " '9': 64,\n",
       " '5': 65,\n",
       " 'c': 66,\n",
       " 'Y': 67,\n",
       " '$': 68,\n",
       " 'r': 69,\n",
       " '1': 70,\n",
       " 'R': 71,\n",
       " 'n': 72,\n",
       " 'X': 73,\n",
       " '@': 74,\n",
       " 'b': 75,\n",
       " 'm': 76,\n",
       " 'p': 77,\n",
       " 'u': 78,\n",
       " '/': 79,\n",
       " '(': 80,\n",
       " '&': 81,\n",
       " '?': 82}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3': 0,\n",
       " '*': 1,\n",
       " 't': 2,\n",
       " 'w': 3,\n",
       " 'V': 4,\n",
       " 'f': 5,\n",
       " ',': 6,\n",
       " '4': 7,\n",
       " 'K': 8,\n",
       " '_': 9,\n",
       " '8': 10,\n",
       " 'T': 11,\n",
       " 'q': 12,\n",
       " 'A': 13,\n",
       " 'Q': 14,\n",
       " '`': 15,\n",
       " 'I': 16,\n",
       " 'J': 17,\n",
       " 'z': 18,\n",
       " 's': 19,\n",
       " 'W': 20,\n",
       " ' ': 21,\n",
       " 'e': 22,\n",
       " 'M': 23,\n",
       " 'F': 24,\n",
       " 'i': 25,\n",
       " 'h': 26,\n",
       " 'o': 27,\n",
       " 'P': 28,\n",
       " 'U': 29,\n",
       " '-': 30,\n",
       " '\"': 31,\n",
       " 'B': 32,\n",
       " '6': 33,\n",
       " 'G': 34,\n",
       " 'O': 35,\n",
       " '%': 36,\n",
       " \"'\": 37,\n",
       " '7': 38,\n",
       " 'v': 39,\n",
       " '0': 40,\n",
       " 'N': 41,\n",
       " ')': 42,\n",
       " 'E': 43,\n",
       " 'L': 44,\n",
       " 'j': 45,\n",
       " 'C': 46,\n",
       " 'S': 47,\n",
       " '\\n': 48,\n",
       " '!': 49,\n",
       " 'd': 50,\n",
       " 'a': 51,\n",
       " 'D': 52,\n",
       " 'g': 53,\n",
       " 'x': 54,\n",
       " ';': 55,\n",
       " '2': 56,\n",
       " 'H': 57,\n",
       " ':': 58,\n",
       " 'y': 59,\n",
       " '.': 60,\n",
       " 'k': 61,\n",
       " 'l': 62,\n",
       " 'Z': 63,\n",
       " '9': 64,\n",
       " '5': 65,\n",
       " 'c': 66,\n",
       " 'Y': 67,\n",
       " '$': 68,\n",
       " 'r': 69,\n",
       " '1': 70,\n",
       " 'R': 71,\n",
       " 'n': 72,\n",
       " 'X': 73,\n",
       " '@': 74,\n",
       " 'b': 75,\n",
       " 'm': 76,\n",
       " 'p': 77,\n",
       " 'u': 78,\n",
       " '/': 79,\n",
       " '(': 80,\n",
       " '&': 81,\n",
       " '?': 82}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_to_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the first 100 characters, make sure everything is peachy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the characters encoded as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([46, 26, 51, 77,  2, 22, 69, 21, 70, 48, 48, 48, 57, 51, 77, 77, 59,\n",
       "       21,  5, 51, 76, 25, 62, 25, 22, 19, 21, 51, 69, 22, 21, 51, 62, 62,\n",
       "       21, 51, 62, 25, 61, 22, 55, 21, 22, 39, 22, 69, 59, 21, 78, 72, 26,\n",
       "       51, 77, 77, 59, 21,  5, 51, 76, 25, 62, 59, 21, 25, 19, 21, 78, 72,\n",
       "       26, 51, 77, 77, 59, 21, 25, 72, 21, 25,  2, 19, 21, 27,  3, 72, 48,\n",
       "        3, 51, 59, 60, 48, 48, 43, 39, 22, 69, 59,  2, 26, 25, 72])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the network is working with individual characters, it's similar to a classification problem in which we are trying to predict the next character from the previous text.  Here's how many 'classes' our network has to pick from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(chars)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making training and validation batches\n",
    "\n",
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the batch size. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the first split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll make my data sets and we can check out what's going on here. Here I'm going to use a batch size of 10 and 50 sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[46, 26, 51, ..., 21, 76, 22],\n",
       "       [21, 51, 76, ..., 19,  2, 51],\n",
       "       [39, 25, 72, ..., 51, 59, 60],\n",
       "       ...,\n",
       "       [55, 21, 75, ..., 27, 39, 25],\n",
       "       [ 2, 21, 25, ..., 19,  2, 51],\n",
       "       [21, 19, 51, ..., 77, 78,  2]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 178650)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the size of this array, we see that we have rows equal to the batch size. When we want to get a batch out of here, we can grab a subset of this array that contains all the rows but has a width equal to the number of steps in the sequence. The first batch looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[46, 26, 51, 77,  2, 22, 69, 21, 70, 48, 48, 48, 57, 51, 77, 77,\n",
       "        59, 21,  5, 51, 76, 25, 62, 25, 22, 19, 21, 51, 69, 22, 21, 51,\n",
       "        62, 62, 21, 51, 62, 25, 61, 22, 55, 21, 22, 39, 22, 69, 59, 21,\n",
       "        78, 72],\n",
       "       [21, 51, 76, 21, 72, 27,  2, 21, 53, 27, 25, 72, 53, 21,  2, 27,\n",
       "        21, 19,  2, 51, 59,  6, 31, 21, 51, 72, 19,  3, 22, 69, 22, 50,\n",
       "        21, 13, 72, 72, 51,  6, 21, 19, 76, 25, 62, 25, 72, 53,  6, 21,\n",
       "        75, 78],\n",
       "       [39, 25, 72, 60, 48, 48, 31, 67, 22, 19,  6, 21, 25,  2, 37, 19,\n",
       "        21, 19, 22,  2,  2, 62, 22, 50, 60, 21, 11, 26, 22, 21, 77, 69,\n",
       "        25, 66, 22, 21, 25, 19, 21, 76, 51, 53, 72, 25,  5, 25, 66, 22,\n",
       "        72,  2],\n",
       "       [72, 21, 50, 78, 69, 25, 72, 53, 21, 26, 25, 19, 21, 66, 27, 72,\n",
       "        39, 22, 69, 19, 51,  2, 25, 27, 72, 21,  3, 25,  2, 26, 21, 26,\n",
       "        25, 19, 48, 75, 69, 27,  2, 26, 22, 69, 21,  3, 51, 19, 21,  2,\n",
       "        26, 25],\n",
       "       [21, 25,  2, 21, 25, 19,  6, 21, 19, 25, 69, 49, 31, 21, 19, 51,\n",
       "        25, 50, 21,  2, 26, 22, 21, 27, 62, 50, 21, 76, 51, 72,  6, 21,\n",
       "        53, 22,  2,  2, 25, 72, 53, 21, 78, 77,  6, 21, 51, 72, 50, 48,\n",
       "        66, 69],\n",
       "       [21, 16,  2, 21,  3, 51, 19, 48, 27, 72, 62, 59, 21,  3, 26, 22,\n",
       "        72, 21,  2, 26, 22, 21, 19, 51, 76, 22, 21, 22, 39, 22, 72, 25,\n",
       "        72, 53, 21, 26, 22, 21, 66, 51, 76, 22, 21,  2, 27, 21,  2, 26,\n",
       "        22, 25],\n",
       "       [26, 22, 72, 21, 66, 27, 76, 22, 21,  5, 27, 69, 21, 76, 22,  6,\n",
       "        31, 21, 19, 26, 22, 21, 19, 51, 25, 50,  6, 21, 51, 72, 50, 21,\n",
       "         3, 22, 72,  2, 21, 75, 51, 66, 61, 21, 25, 72,  2, 27, 21,  2,\n",
       "        26, 22],\n",
       "       [55, 21, 75, 78,  2, 21, 72, 27,  3, 21, 19, 26, 22, 21,  3, 27,\n",
       "        78, 62, 50, 21, 69, 22, 51, 50, 25, 62, 59, 21, 26, 51, 39, 22,\n",
       "        21, 19, 51, 66, 69, 25,  5, 25, 66, 22, 50,  6, 21, 72, 27,  2,\n",
       "        21, 76],\n",
       "       [ 2, 21, 25, 19, 72, 37,  2, 60, 21, 11, 26, 22, 59, 37, 69, 22,\n",
       "        21, 77, 69, 27, 77, 69, 25, 22,  2, 27, 69, 19, 21, 27,  5, 21,\n",
       "        51, 21, 19, 27, 69,  2,  6, 48, 75, 78,  2, 21,  3, 22, 37, 69,\n",
       "        22, 21],\n",
       "       [21, 19, 51, 25, 50, 21,  2, 27, 21, 26, 22, 69, 19, 22, 62,  5,\n",
       "         6, 21, 51, 72, 50, 21, 75, 22, 53, 51, 72, 21, 51, 53, 51, 25,\n",
       "        72, 21,  5, 69, 27, 76, 21,  2, 26, 22, 21, 75, 22, 53, 25, 72,\n",
       "        72, 25]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll write another function to grab batches out of the arrays made by `split_data`. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "Below is a function where I build the graph for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "    \n",
    "    # When we're using this network for sampling later, we'll be passing in\n",
    "    # one character at a time, so providing an option for that\n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # One-hot encoding the input and target characters\n",
    "    x_one_hot = tf.one_hot(inputs, num_classes)\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "\n",
    "    ### Build the RNN layers\n",
    "    # Use a basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "#     stacked_rnn = []\n",
    "#         for iiLyr in range():\n",
    "#             stacked_rnn.append(tf.nn.rnn_cell.LSTMCell(num_units=lstm_size, state_is_tuple=True))\n",
    "    \n",
    "    #cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    initial_state = lstm.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    ### Run the data through the RNN layers\n",
    "    # This makes a list where each element is on step in the sequence\n",
    "    rnn_inputs = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(x_one_hot, num_steps, 1)]\n",
    "    \n",
    "    # Run each sequence step through the RNN and collect the outputs\n",
    "    outputs, state = tf.contrib.rnn.static_rnn(lstm, rnn_inputs, initial_state=initial_state)\n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one output row for each step for each batch\n",
    "    seq_output = tf.concat(outputs, axis=1)\n",
    "    output = tf.reshape(seq_output, [-1, lstm_size])\n",
    "    \n",
    "    # Now connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and batch\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    preds = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    # Reshape the targets to match the logits\n",
    "    y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    # Export the nodes\n",
    "    # NOTE: I'm using a namedtuple here because I think they are cool\n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. \n",
    "\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `num_steps` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lstm_size` - The number of units in the hidden layers.\n",
    "* `num_layers` - Number of hidden LSTM layers to use\n",
    "* `learning_rate` - Learning rate for training\n",
    "* `keep_prob` - The dropout keep probability when training. If you're network is overfitting, try decreasing this.\n",
    "\n",
    "Here's some good advice from Andrej Karpathy on training the network. I'm going to write it in here for your benefit, but also link to [where it originally came from](https://github.com/karpathy/char-rnn#tips-and-tricks).\n",
    "\n",
    "> ## Tips and Tricks\n",
    "\n",
    ">### Monitoring Validation Loss vs. Training Loss\n",
    ">If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "\n",
    "> - If your training loss is much lower than validation loss then this means the network might be **overfitting**. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
    "> - If your training/validation loss are about equal then your model is **underfitting**. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    "> ### Approximate number of parameters\n",
    "\n",
    "> The two most important parameters that control the model are `lstm_size` and `num_layers`. I would advise that you always use `num_layers` of either 2/3. The `lstm_size` can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "> - The number of parameters in your model. This is printed when you start training.\n",
    "> - The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    ">These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "> - I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make `lstm_size` larger.\n",
    "> - I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\n",
    "\n",
    "> ### Best models strategy\n",
    "\n",
    ">The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    ">It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
    "\n",
    ">By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100 \n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "keep_prob = 10.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file checkpoints already exists.\n"
     ]
    }
   ],
   "source": [
    "!mkdir checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint.\n",
    "\n",
    "Here I'm saving checkpoints with the format\n",
    "\n",
    "`i{iteration number}_l{# hidden layer units}_v{validation loss}.ckpt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 1/3560 Training loss: 4.4241 0.4827 sec/batch\n",
      "Epoch 1/20  Iteration 2/3560 Training loss: 4.3984 0.1415 sec/batch\n",
      "Epoch 1/20  Iteration 3/3560 Training loss: 4.3531 0.1861 sec/batch\n",
      "Epoch 1/20  Iteration 4/3560 Training loss: 4.1896 0.0971 sec/batch\n",
      "Epoch 1/20  Iteration 5/3560 Training loss: 4.0333 0.0980 sec/batch\n",
      "Epoch 1/20  Iteration 6/3560 Training loss: 3.9268 0.2053 sec/batch\n",
      "Epoch 1/20  Iteration 7/3560 Training loss: 3.8355 0.1948 sec/batch\n",
      "Epoch 1/20  Iteration 8/3560 Training loss: 3.7560 0.1085 sec/batch\n",
      "Epoch 1/20  Iteration 9/3560 Training loss: 3.6924 0.1978 sec/batch\n",
      "Epoch 1/20  Iteration 10/3560 Training loss: 3.6394 0.1140 sec/batch\n",
      "Epoch 1/20  Iteration 11/3560 Training loss: 3.5931 0.1870 sec/batch\n",
      "Epoch 1/20  Iteration 12/3560 Training loss: 3.5547 0.0970 sec/batch\n",
      "Epoch 1/20  Iteration 13/3560 Training loss: 3.5219 0.0990 sec/batch\n",
      "Epoch 1/20  Iteration 14/3560 Training loss: 3.4946 0.1030 sec/batch\n",
      "Epoch 1/20  Iteration 15/3560 Training loss: 3.4699 0.0980 sec/batch\n",
      "Epoch 1/20  Iteration 16/3560 Training loss: 3.4483 0.2117 sec/batch\n",
      "Epoch 1/20  Iteration 17/3560 Training loss: 3.4280 0.1227 sec/batch\n",
      "Epoch 1/20  Iteration 18/3560 Training loss: 3.4115 0.1715 sec/batch\n",
      "Epoch 1/20  Iteration 19/3560 Training loss: 3.3956 0.2018 sec/batch\n",
      "Epoch 1/20  Iteration 20/3560 Training loss: 3.3795 0.0983 sec/batch\n",
      "Epoch 1/20  Iteration 21/3560 Training loss: 3.3660 0.1912 sec/batch\n",
      "Epoch 1/20  Iteration 22/3560 Training loss: 3.3537 0.1323 sec/batch\n",
      "Epoch 1/20  Iteration 23/3560 Training loss: 3.3418 0.1610 sec/batch\n",
      "Epoch 1/20  Iteration 24/3560 Training loss: 3.3310 0.0990 sec/batch\n",
      "Epoch 1/20  Iteration 25/3560 Training loss: 3.3206 0.2020 sec/batch\n",
      "Epoch 1/20  Iteration 26/3560 Training loss: 3.3118 0.2106 sec/batch\n",
      "Epoch 1/20  Iteration 27/3560 Training loss: 3.3036 0.1088 sec/batch\n",
      "Epoch 1/20  Iteration 28/3560 Training loss: 3.2950 0.1690 sec/batch\n",
      "Epoch 1/20  Iteration 29/3560 Training loss: 3.2872 0.0960 sec/batch\n",
      "Epoch 1/20  Iteration 30/3560 Training loss: 3.2801 0.0880 sec/batch\n",
      "Epoch 1/20  Iteration 31/3560 Training loss: 3.2741 0.1140 sec/batch\n",
      "Epoch 1/20  Iteration 32/3560 Training loss: 3.2675 0.2000 sec/batch\n",
      "Epoch 1/20  Iteration 33/3560 Training loss: 3.2609 0.1000 sec/batch\n",
      "Epoch 1/20  Iteration 34/3560 Training loss: 3.2553 0.0990 sec/batch\n",
      "Epoch 1/20  Iteration 35/3560 Training loss: 3.2494 0.2020 sec/batch\n",
      "Epoch 1/20  Iteration 36/3560 Training loss: 3.2444 0.1050 sec/batch\n",
      "Epoch 1/20  Iteration 37/3560 Training loss: 3.2387 0.0980 sec/batch\n",
      "Epoch 1/20  Iteration 38/3560 Training loss: 3.2332 0.2010 sec/batch\n",
      "Epoch 1/20  Iteration 39/3560 Training loss: 3.2280 0.1016 sec/batch\n",
      "Epoch 1/20  Iteration 40/3560 Training loss: 3.2232 0.0980 sec/batch\n",
      "Epoch 1/20  Iteration 41/3560 Training loss: 3.2183 0.1000 sec/batch\n",
      "Epoch 1/20  Iteration 42/3560 Training loss: 3.2138 0.0990 sec/batch\n",
      "Epoch 1/20  Iteration 43/3560 Training loss: 3.2093 0.1020 sec/batch\n",
      "Epoch 1/20  Iteration 44/3560 Training loss: 3.2049 0.1000 sec/batch\n",
      "Epoch 1/20  Iteration 45/3560 Training loss: 3.2006 0.2010 sec/batch\n",
      "Epoch 1/20  Iteration 46/3560 Training loss: 3.1967 0.2006 sec/batch\n",
      "Epoch 1/20  Iteration 47/3560 Training loss: 3.1931 0.0910 sec/batch\n",
      "Epoch 1/20  Iteration 48/3560 Training loss: 3.1897 0.1070 sec/batch\n",
      "Epoch 1/20  Iteration 49/3560 Training loss: 3.1863 0.2023 sec/batch\n",
      "Epoch 1/20  Iteration 50/3560 Training loss: 3.1830 0.1010 sec/batch\n",
      "Epoch 1/20  Iteration 51/3560 Training loss: 3.1796 0.0990 sec/batch\n",
      "Epoch 1/20  Iteration 52/3560 Training loss: 3.1761 0.1010 sec/batch\n",
      "Epoch 1/20  Iteration 53/3560 Training loss: 3.1728 0.0990 sec/batch\n",
      "Epoch 1/20  Iteration 54/3560 Training loss: 3.1694 0.2012 sec/batch\n",
      "Epoch 1/20  Iteration 55/3560 Training loss: 3.1662 0.2010 sec/batch\n",
      "Epoch 1/20  Iteration 56/3560 Training loss: 3.1627 0.2001 sec/batch\n",
      "Epoch 1/20  Iteration 57/3560 Training loss: 3.1595 0.0890 sec/batch\n",
      "Epoch 1/20  Iteration 58/3560 Training loss: 3.1564 0.1120 sec/batch\n",
      "Epoch 1/20  Iteration 59/3560 Training loss: 3.1531 0.1990 sec/batch\n",
      "Epoch 1/20  Iteration 60/3560 Training loss: 3.1501 0.1070 sec/batch\n",
      "Epoch 1/20  Iteration 61/3560 Training loss: 3.1471 0.0950 sec/batch\n",
      "Epoch 1/20  Iteration 62/3560 Training loss: 3.1444 0.0990 sec/batch\n",
      "Epoch 1/20  Iteration 63/3560 Training loss: 3.1419 0.1000 sec/batch\n",
      "Epoch 1/20  Iteration 64/3560 Training loss: 3.1386 0.0980 sec/batch\n",
      "Epoch 1/20  Iteration 65/3560 Training loss: 3.1354 0.1010 sec/batch\n",
      "Epoch 1/20  Iteration 66/3560 Training loss: 3.1327 0.1010 sec/batch\n",
      "Epoch 1/20  Iteration 67/3560 Training loss: 3.1298 0.1950 sec/batch\n",
      "Epoch 1/20  Iteration 68/3560 Training loss: 3.1263 0.1030 sec/batch\n",
      "Epoch 1/20  Iteration 69/3560 Training loss: 3.1230 0.1020 sec/batch\n",
      "Epoch 1/20  Iteration 70/3560 Training loss: 3.1202 0.1970 sec/batch\n",
      "Epoch 1/20  Iteration 71/3560 Training loss: 3.1172 0.1030 sec/batch\n",
      "Epoch 1/20  Iteration 72/3560 Training loss: 3.1145 0.0901 sec/batch\n",
      "Epoch 1/20  Iteration 73/3560 Training loss: 3.1115 0.1060 sec/batch\n",
      "Epoch 1/20  Iteration 74/3560 Training loss: 3.1084 0.0980 sec/batch\n",
      "Epoch 1/20  Iteration 75/3560 Training loss: 3.1055 0.2040 sec/batch\n",
      "Epoch 1/20  Iteration 76/3560 Training loss: 3.1027 0.1970 sec/batch\n",
      "Epoch 1/20  Iteration 77/3560 Training loss: 3.0997 0.2030 sec/batch\n",
      "Epoch 1/20  Iteration 78/3560 Training loss: 3.0967 0.2000 sec/batch\n",
      "Epoch 1/20  Iteration 79/3560 Training loss: 3.0935 0.1990 sec/batch\n",
      "Epoch 1/20  Iteration 80/3560 Training loss: 3.0903 0.0935 sec/batch\n",
      "Epoch 1/20  Iteration 81/3560 Training loss: 3.0871 0.1090 sec/batch\n",
      "Epoch 1/20  Iteration 82/3560 Training loss: 3.0840 0.1000 sec/batch\n",
      "Epoch 1/20  Iteration 83/3560 Training loss: 3.0808 0.1000 sec/batch\n",
      "Epoch 1/20  Iteration 84/3560 Training loss: 3.0776 0.0910 sec/batch\n",
      "Epoch 1/20  Iteration 85/3560 Training loss: 3.0740 0.1070 sec/batch\n",
      "Epoch 1/20  Iteration 86/3560 Training loss: 3.0705 0.0890 sec/batch\n",
      "Epoch 1/20  Iteration 87/3560 Training loss: 3.0670 0.1110 sec/batch\n",
      "Epoch 1/20  Iteration 88/3560 Training loss: 3.0636 0.1000 sec/batch\n",
      "Epoch 1/20  Iteration 89/3560 Training loss: 3.0602 0.1020 sec/batch\n",
      "Epoch 1/20  Iteration 90/3560 Training loss: 3.0568 0.0970 sec/batch\n",
      "Epoch 1/20  Iteration 91/3560 Training loss: 3.0534 0.1020 sec/batch\n",
      "Epoch 1/20  Iteration 92/3560 Training loss: 3.0501 0.2030 sec/batch\n",
      "Epoch 1/20  Iteration 93/3560 Training loss: 3.0468 0.0970 sec/batch\n",
      "Epoch 1/20  Iteration 94/3560 Training loss: 3.0433 0.0980 sec/batch\n",
      "Epoch 1/20  Iteration 95/3560 Training loss: 3.0396 0.1033 sec/batch\n",
      "Epoch 1/20  Iteration 96/3560 Training loss: 3.0361 0.0870 sec/batch\n",
      "Epoch 1/20  Iteration 97/3560 Training loss: 3.0327 0.1100 sec/batch\n",
      "Epoch 1/20  Iteration 98/3560 Training loss: 3.0291 0.1990 sec/batch\n",
      "Epoch 1/20  Iteration 99/3560 Training loss: 3.0256 0.1040 sec/batch\n",
      "Epoch 1/20  Iteration 100/3560 Training loss: 3.0221 0.0880 sec/batch\n",
      "Epoch 1/20  Iteration 101/3560 Training loss: 3.0187 0.1085 sec/batch\n",
      "Epoch 1/20  Iteration 102/3560 Training loss: 3.0150 0.1010 sec/batch\n",
      "Epoch 1/20  Iteration 103/3560 Training loss: 3.0113 0.0875 sec/batch\n",
      "Epoch 1/20  Iteration 104/3560 Training loss: 3.0076 0.1120 sec/batch\n",
      "Epoch 1/20  Iteration 105/3560 Training loss: 3.0039 0.0950 sec/batch\n",
      "Epoch 1/20  Iteration 106/3560 Training loss: 3.0002 0.2000 sec/batch\n",
      "Epoch 1/20  Iteration 107/3560 Training loss: 2.9965 0.2060 sec/batch\n",
      "Epoch 1/20  Iteration 108/3560 Training loss: 2.9935 0.1993 sec/batch\n",
      "Epoch 1/20  Iteration 109/3560 Training loss: 2.9906 0.2026 sec/batch\n",
      "Epoch 1/20  Iteration 110/3560 Training loss: 2.9869 0.1218 sec/batch\n",
      "Epoch 1/20  Iteration 111/3560 Training loss: 2.9835 0.1730 sec/batch\n",
      "Epoch 1/20  Iteration 112/3560 Training loss: 2.9801 0.0940 sec/batch\n",
      "Epoch 1/20  Iteration 113/3560 Training loss: 2.9767 0.0880 sec/batch\n",
      "Epoch 1/20  Iteration 114/3560 Training loss: 2.9730 0.1090 sec/batch\n",
      "Epoch 1/20  Iteration 115/3560 Training loss: 2.9694 0.0890 sec/batch\n",
      "Epoch 1/20  Iteration 116/3560 Training loss: 2.9658 0.1110 sec/batch\n",
      "Epoch 1/20  Iteration 117/3560 Training loss: 2.9622 0.1000 sec/batch\n",
      "Epoch 1/20  Iteration 118/3560 Training loss: 2.9588 0.0885 sec/batch\n",
      "Epoch 1/20  Iteration 119/3560 Training loss: 2.9555 0.1188 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 120/3560 Training loss: 2.9521 0.1920 sec/batch\n",
      "Epoch 1/20  Iteration 121/3560 Training loss: 2.9489 0.0980 sec/batch\n",
      "Epoch 1/20  Iteration 122/3560 Training loss: 2.9454 0.1000 sec/batch\n",
      "Epoch 1/20  Iteration 123/3560 Training loss: 2.9420 0.1077 sec/batch\n",
      "Epoch 1/20  Iteration 124/3560 Training loss: 2.9386 0.1953 sec/batch\n",
      "Epoch 1/20  Iteration 125/3560 Training loss: 2.9352 0.1896 sec/batch\n",
      "Epoch 1/20  Iteration 126/3560 Training loss: 2.9316 0.0880 sec/batch\n",
      "Epoch 1/20  Iteration 127/3560 Training loss: 2.9283 0.1100 sec/batch\n",
      "Epoch 1/20  Iteration 128/3560 Training loss: 2.9251 0.0990 sec/batch\n",
      "Epoch 1/20  Iteration 129/3560 Training loss: 2.9219 0.0880 sec/batch\n",
      "Epoch 1/20  Iteration 130/3560 Training loss: 2.9193 0.0890 sec/batch\n",
      "Epoch 1/20  Iteration 131/3560 Training loss: 2.9160 0.1240 sec/batch\n",
      "Epoch 1/20  Iteration 132/3560 Training loss: 2.9129 0.0980 sec/batch\n",
      "Epoch 1/20  Iteration 133/3560 Training loss: 2.9098 0.0910 sec/batch\n",
      "Epoch 1/20  Iteration 134/3560 Training loss: 2.9067 0.1090 sec/batch\n",
      "Epoch 1/20  Iteration 135/3560 Training loss: 2.9035 0.1000 sec/batch\n",
      "Epoch 1/20  Iteration 136/3560 Training loss: 2.9003 0.0990 sec/batch\n",
      "Epoch 1/20  Iteration 137/3560 Training loss: 2.8972 0.2030 sec/batch\n",
      "Epoch 1/20  Iteration 138/3560 Training loss: 2.8941 0.1030 sec/batch\n",
      "Epoch 1/20  Iteration 139/3560 Training loss: 2.8911 0.0980 sec/batch\n",
      "Epoch 1/20  Iteration 140/3560 Training loss: 2.8880 0.0990 sec/batch\n",
      "Epoch 1/20  Iteration 141/3560 Training loss: 2.8850 0.0990 sec/batch\n",
      "Epoch 1/20  Iteration 142/3560 Training loss: 2.8819 0.1990 sec/batch\n",
      "Epoch 1/20  Iteration 143/3560 Training loss: 2.8788 0.1020 sec/batch\n",
      "Epoch 1/20  Iteration 144/3560 Training loss: 2.8757 0.0980 sec/batch\n",
      "Epoch 1/20  Iteration 145/3560 Training loss: 2.8726 0.1000 sec/batch\n",
      "Epoch 1/20  Iteration 146/3560 Training loss: 2.8697 0.1000 sec/batch\n",
      "Epoch 1/20  Iteration 147/3560 Training loss: 2.8667 0.1026 sec/batch\n",
      "Epoch 1/20  Iteration 148/3560 Training loss: 2.8638 0.1010 sec/batch\n",
      "Epoch 1/20  Iteration 149/3560 Training loss: 2.8607 0.0860 sec/batch\n",
      "Epoch 1/20  Iteration 150/3560 Training loss: 2.8576 0.1100 sec/batch\n",
      "Epoch 1/20  Iteration 151/3560 Training loss: 2.8547 0.1010 sec/batch\n",
      "Epoch 1/20  Iteration 152/3560 Training loss: 2.8520 0.0980 sec/batch\n",
      "Epoch 1/20  Iteration 153/3560 Training loss: 2.8492 0.1000 sec/batch\n",
      "Epoch 1/20  Iteration 154/3560 Training loss: 2.8464 0.0989 sec/batch\n",
      "Epoch 1/20  Iteration 155/3560 Training loss: 2.8434 0.1020 sec/batch\n",
      "Epoch 1/20  Iteration 156/3560 Training loss: 2.8405 0.1000 sec/batch\n",
      "Epoch 1/20  Iteration 157/3560 Training loss: 2.8375 0.1000 sec/batch\n",
      "Epoch 1/20  Iteration 158/3560 Training loss: 2.8346 0.1020 sec/batch\n",
      "Epoch 1/20  Iteration 159/3560 Training loss: 2.8315 0.0880 sec/batch\n",
      "Epoch 1/20  Iteration 160/3560 Training loss: 2.8287 0.1100 sec/batch\n",
      "Epoch 1/20  Iteration 161/3560 Training loss: 2.8260 0.1070 sec/batch\n",
      "Epoch 1/20  Iteration 162/3560 Training loss: 2.8230 0.0870 sec/batch\n",
      "Epoch 1/20  Iteration 163/3560 Training loss: 2.8201 0.0880 sec/batch\n",
      "Epoch 1/20  Iteration 164/3560 Training loss: 2.8172 0.1190 sec/batch\n",
      "Epoch 1/20  Iteration 165/3560 Training loss: 2.8145 0.0980 sec/batch\n",
      "Epoch 1/20  Iteration 166/3560 Training loss: 2.8118 0.0890 sec/batch\n",
      "Epoch 1/20  Iteration 167/3560 Training loss: 2.8093 0.1114 sec/batch\n",
      "Epoch 1/20  Iteration 168/3560 Training loss: 2.8072 0.2053 sec/batch\n",
      "Epoch 1/20  Iteration 169/3560 Training loss: 2.8048 0.1970 sec/batch\n",
      "Epoch 1/20  Iteration 170/3560 Training loss: 2.8022 0.1020 sec/batch\n",
      "Epoch 1/20  Iteration 171/3560 Training loss: 2.7997 0.1000 sec/batch\n",
      "Epoch 1/20  Iteration 172/3560 Training loss: 2.7974 0.1990 sec/batch\n",
      "Epoch 1/20  Iteration 173/3560 Training loss: 2.7952 0.1990 sec/batch\n",
      "Epoch 1/20  Iteration 174/3560 Training loss: 2.7929 0.1005 sec/batch\n",
      "Epoch 1/20  Iteration 175/3560 Training loss: 2.7907 0.0950 sec/batch\n",
      "Epoch 1/20  Iteration 176/3560 Training loss: 2.7883 0.0870 sec/batch\n",
      "Epoch 1/20  Iteration 177/3560 Training loss: 2.7858 0.0850 sec/batch\n",
      "Epoch 1/20  Iteration 178/3560 Training loss: 2.7832 0.0860 sec/batch\n",
      "Epoch 2/20  Iteration 179/3560 Training loss: 2.4490 0.0860 sec/batch\n",
      "Epoch 2/20  Iteration 180/3560 Training loss: 2.3785 0.1530 sec/batch\n",
      "Epoch 2/20  Iteration 181/3560 Training loss: 2.3576 0.0850 sec/batch\n",
      "Epoch 2/20  Iteration 182/3560 Training loss: 2.3506 0.0850 sec/batch\n",
      "Epoch 2/20  Iteration 183/3560 Training loss: 2.3455 0.1300 sec/batch\n",
      "Epoch 2/20  Iteration 184/3560 Training loss: 2.3399 0.0860 sec/batch\n",
      "Epoch 2/20  Iteration 185/3560 Training loss: 2.3380 0.0860 sec/batch\n",
      "Epoch 2/20  Iteration 186/3560 Training loss: 2.3368 0.1340 sec/batch\n",
      "Epoch 2/20  Iteration 187/3560 Training loss: 2.3360 0.1986 sec/batch\n",
      "Epoch 2/20  Iteration 188/3560 Training loss: 2.3339 0.1996 sec/batch\n",
      "Epoch 2/20  Iteration 189/3560 Training loss: 2.3303 0.1970 sec/batch\n",
      "Epoch 2/20  Iteration 190/3560 Training loss: 2.3283 0.2005 sec/batch\n",
      "Epoch 2/20  Iteration 191/3560 Training loss: 2.3270 0.0930 sec/batch\n",
      "Epoch 2/20  Iteration 192/3560 Training loss: 2.3280 0.1080 sec/batch\n",
      "Epoch 2/20  Iteration 193/3560 Training loss: 2.3274 0.1000 sec/batch\n",
      "Epoch 2/20  Iteration 194/3560 Training loss: 2.3294 0.1000 sec/batch\n",
      "Epoch 2/20  Iteration 195/3560 Training loss: 2.3295 0.0995 sec/batch\n",
      "Epoch 2/20  Iteration 196/3560 Training loss: 2.3307 0.1091 sec/batch\n",
      "Epoch 2/20  Iteration 197/3560 Training loss: 2.3302 0.2018 sec/batch\n",
      "Epoch 2/20  Iteration 198/3560 Training loss: 2.3282 0.1886 sec/batch\n",
      "Epoch 2/20  Iteration 199/3560 Training loss: 2.3269 0.0900 sec/batch\n",
      "Epoch 2/20  Iteration 200/3560 Training loss: 2.3270 0.2107 sec/batch\n",
      "Validation loss: 2.2940805 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 201/3560 Training loss: 2.3258 0.1430 sec/batch\n",
      "Epoch 2/20  Iteration 202/3560 Training loss: 2.3242 0.1000 sec/batch\n",
      "Epoch 2/20  Iteration 203/3560 Training loss: 2.3226 0.0880 sec/batch\n",
      "Epoch 2/20  Iteration 204/3560 Training loss: 2.3213 0.1110 sec/batch\n",
      "Epoch 2/20  Iteration 205/3560 Training loss: 2.3197 0.0880 sec/batch\n",
      "Epoch 2/20  Iteration 206/3560 Training loss: 2.3183 0.1150 sec/batch\n",
      "Epoch 2/20  Iteration 207/3560 Training loss: 2.3177 0.1965 sec/batch\n",
      "Epoch 2/20  Iteration 208/3560 Training loss: 2.3169 0.0960 sec/batch\n",
      "Epoch 2/20  Iteration 209/3560 Training loss: 2.3162 0.1020 sec/batch\n",
      "Epoch 2/20  Iteration 210/3560 Training loss: 2.3146 0.2010 sec/batch\n",
      "Epoch 2/20  Iteration 211/3560 Training loss: 2.3128 0.1951 sec/batch\n",
      "Epoch 2/20  Iteration 212/3560 Training loss: 2.3118 0.0920 sec/batch\n",
      "Epoch 2/20  Iteration 213/3560 Training loss: 2.3105 0.1060 sec/batch\n",
      "Epoch 2/20  Iteration 214/3560 Training loss: 2.3093 0.1015 sec/batch\n",
      "Epoch 2/20  Iteration 215/3560 Training loss: 2.3078 0.1980 sec/batch\n",
      "Epoch 2/20  Iteration 216/3560 Training loss: 2.3057 0.0900 sec/batch\n",
      "Epoch 2/20  Iteration 217/3560 Training loss: 2.3038 0.1100 sec/batch\n",
      "Epoch 2/20  Iteration 218/3560 Training loss: 2.3020 0.1010 sec/batch\n",
      "Epoch 2/20  Iteration 219/3560 Training loss: 2.3004 0.0980 sec/batch\n",
      "Epoch 2/20  Iteration 220/3560 Training loss: 2.2991 0.1020 sec/batch\n",
      "Epoch 2/20  Iteration 221/3560 Training loss: 2.2974 0.0980 sec/batch\n",
      "Epoch 2/20  Iteration 222/3560 Training loss: 2.2957 0.1000 sec/batch\n",
      "Epoch 2/20  Iteration 223/3560 Training loss: 2.2942 0.0980 sec/batch\n",
      "Epoch 2/20  Iteration 224/3560 Training loss: 2.2919 0.2000 sec/batch\n",
      "Epoch 2/20  Iteration 225/3560 Training loss: 2.2910 0.1020 sec/batch\n",
      "Epoch 2/20  Iteration 226/3560 Training loss: 2.2895 0.0980 sec/batch\n",
      "Epoch 2/20  Iteration 227/3560 Training loss: 2.2882 0.0900 sec/batch\n",
      "Epoch 2/20  Iteration 228/3560 Training loss: 2.2876 0.1100 sec/batch\n",
      "Epoch 2/20  Iteration 229/3560 Training loss: 2.2859 0.1000 sec/batch\n",
      "Epoch 2/20  Iteration 230/3560 Training loss: 2.2851 0.2066 sec/batch\n",
      "Epoch 2/20  Iteration 231/3560 Training loss: 2.2838 0.2014 sec/batch\n",
      "Epoch 2/20  Iteration 232/3560 Training loss: 2.2823 0.2111 sec/batch\n",
      "Epoch 2/20  Iteration 233/3560 Training loss: 2.2811 0.1963 sec/batch\n",
      "Epoch 2/20  Iteration 234/3560 Training loss: 2.2801 0.0880 sec/batch\n",
      "Epoch 2/20  Iteration 235/3560 Training loss: 2.2790 0.1130 sec/batch\n",
      "Epoch 2/20  Iteration 236/3560 Training loss: 2.2776 0.1010 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20  Iteration 237/3560 Training loss: 2.2763 0.1010 sec/batch\n",
      "Epoch 2/20  Iteration 238/3560 Training loss: 2.2758 0.0870 sec/batch\n",
      "Epoch 2/20  Iteration 239/3560 Training loss: 2.2746 0.1100 sec/batch\n",
      "Epoch 2/20  Iteration 240/3560 Training loss: 2.2739 0.0890 sec/batch\n",
      "Epoch 2/20  Iteration 241/3560 Training loss: 2.2732 0.0870 sec/batch\n",
      "Epoch 2/20  Iteration 242/3560 Training loss: 2.2722 0.1260 sec/batch\n",
      "Epoch 2/20  Iteration 243/3560 Training loss: 2.2712 0.0890 sec/batch\n",
      "Epoch 2/20  Iteration 244/3560 Training loss: 2.2705 0.1110 sec/batch\n",
      "Epoch 2/20  Iteration 245/3560 Training loss: 2.2694 0.1000 sec/batch\n",
      "Epoch 2/20  Iteration 246/3560 Training loss: 2.2681 0.1173 sec/batch\n",
      "Epoch 2/20  Iteration 247/3560 Training loss: 2.2669 0.1101 sec/batch\n",
      "Epoch 2/20  Iteration 248/3560 Training loss: 2.2662 0.1771 sec/batch\n",
      "Epoch 2/20  Iteration 249/3560 Training loss: 2.2655 0.2032 sec/batch\n",
      "Epoch 2/20  Iteration 250/3560 Training loss: 2.2647 0.1920 sec/batch\n",
      "Epoch 2/20  Iteration 251/3560 Training loss: 2.2640 0.0970 sec/batch\n",
      "Epoch 2/20  Iteration 252/3560 Training loss: 2.2629 0.1102 sec/batch\n",
      "Epoch 2/20  Iteration 253/3560 Training loss: 2.2620 0.1393 sec/batch\n",
      "Epoch 2/20  Iteration 254/3560 Training loss: 2.2616 0.0945 sec/batch\n",
      "Epoch 2/20  Iteration 255/3560 Training loss: 2.2606 0.1570 sec/batch\n",
      "Epoch 2/20  Iteration 256/3560 Training loss: 2.2600 0.0890 sec/batch\n",
      "Epoch 2/20  Iteration 257/3560 Training loss: 2.2588 0.1110 sec/batch\n",
      "Epoch 2/20  Iteration 258/3560 Training loss: 2.2578 0.0890 sec/batch\n",
      "Epoch 2/20  Iteration 259/3560 Training loss: 2.2567 0.1120 sec/batch\n",
      "Epoch 2/20  Iteration 260/3560 Training loss: 2.2559 0.0990 sec/batch\n",
      "Epoch 2/20  Iteration 261/3560 Training loss: 2.2548 0.0980 sec/batch\n",
      "Epoch 2/20  Iteration 262/3560 Training loss: 2.2538 0.0890 sec/batch\n",
      "Epoch 2/20  Iteration 263/3560 Training loss: 2.2528 0.1120 sec/batch\n",
      "Epoch 2/20  Iteration 264/3560 Training loss: 2.2519 0.2020 sec/batch\n",
      "Epoch 2/20  Iteration 265/3560 Training loss: 2.2510 0.1980 sec/batch\n",
      "Epoch 2/20  Iteration 266/3560 Training loss: 2.2501 0.1000 sec/batch\n",
      "Epoch 2/20  Iteration 267/3560 Training loss: 2.2491 0.0990 sec/batch\n",
      "Epoch 2/20  Iteration 268/3560 Training loss: 2.2484 0.1010 sec/batch\n",
      "Epoch 2/20  Iteration 269/3560 Training loss: 2.2475 0.2000 sec/batch\n",
      "Epoch 2/20  Iteration 270/3560 Training loss: 2.2467 0.0880 sec/batch\n",
      "Epoch 2/20  Iteration 271/3560 Training loss: 2.2456 0.1104 sec/batch\n",
      "Epoch 2/20  Iteration 272/3560 Training loss: 2.2446 0.1021 sec/batch\n",
      "Epoch 2/20  Iteration 273/3560 Training loss: 2.2436 0.0970 sec/batch\n",
      "Epoch 2/20  Iteration 274/3560 Training loss: 2.2427 0.0990 sec/batch\n",
      "Epoch 2/20  Iteration 275/3560 Training loss: 2.2418 0.1020 sec/batch\n",
      "Epoch 2/20  Iteration 276/3560 Training loss: 2.2408 0.2000 sec/batch\n",
      "Epoch 2/20  Iteration 277/3560 Training loss: 2.2398 0.0994 sec/batch\n",
      "Epoch 2/20  Iteration 278/3560 Training loss: 2.2387 0.1010 sec/batch\n",
      "Epoch 2/20  Iteration 279/3560 Training loss: 2.2380 0.1010 sec/batch\n",
      "Epoch 2/20  Iteration 280/3560 Training loss: 2.2372 0.1020 sec/batch\n",
      "Epoch 2/20  Iteration 281/3560 Training loss: 2.2361 0.1010 sec/batch\n",
      "Epoch 2/20  Iteration 282/3560 Training loss: 2.2352 0.0990 sec/batch\n",
      "Epoch 2/20  Iteration 283/3560 Training loss: 2.2342 0.0990 sec/batch\n",
      "Epoch 2/20  Iteration 284/3560 Training loss: 2.2334 0.0860 sec/batch\n",
      "Epoch 2/20  Iteration 285/3560 Training loss: 2.2325 0.1130 sec/batch\n",
      "Epoch 2/20  Iteration 286/3560 Training loss: 2.2319 0.1010 sec/batch\n",
      "Epoch 2/20  Iteration 287/3560 Training loss: 2.2312 0.0990 sec/batch\n",
      "Epoch 2/20  Iteration 288/3560 Training loss: 2.2302 0.1015 sec/batch\n",
      "Epoch 2/20  Iteration 289/3560 Training loss: 2.2294 0.0945 sec/batch\n",
      "Epoch 2/20  Iteration 290/3560 Training loss: 2.2287 0.1050 sec/batch\n",
      "Epoch 2/20  Iteration 291/3560 Training loss: 2.2278 0.1010 sec/batch\n",
      "Epoch 2/20  Iteration 292/3560 Training loss: 2.2269 0.1010 sec/batch\n",
      "Epoch 2/20  Iteration 293/3560 Training loss: 2.2260 0.2020 sec/batch\n",
      "Epoch 2/20  Iteration 294/3560 Training loss: 2.2249 0.1010 sec/batch\n",
      "Epoch 2/20  Iteration 295/3560 Training loss: 2.2241 0.1010 sec/batch\n",
      "Epoch 2/20  Iteration 296/3560 Training loss: 2.2233 0.2100 sec/batch\n",
      "Epoch 2/20  Iteration 297/3560 Training loss: 2.2226 0.1831 sec/batch\n",
      "Epoch 2/20  Iteration 298/3560 Training loss: 2.2219 0.1930 sec/batch\n",
      "Epoch 2/20  Iteration 299/3560 Training loss: 2.2213 0.0990 sec/batch\n",
      "Epoch 2/20  Iteration 300/3560 Training loss: 2.2204 0.0990 sec/batch\n",
      "Epoch 2/20  Iteration 301/3560 Training loss: 2.2195 0.1000 sec/batch\n",
      "Epoch 2/20  Iteration 302/3560 Training loss: 2.2190 0.1343 sec/batch\n",
      "Epoch 2/20  Iteration 303/3560 Training loss: 2.2183 0.1595 sec/batch\n",
      "Epoch 2/20  Iteration 304/3560 Training loss: 2.2174 0.1013 sec/batch\n",
      "Epoch 2/20  Iteration 305/3560 Training loss: 2.2167 0.1000 sec/batch\n",
      "Epoch 2/20  Iteration 306/3560 Training loss: 2.2161 0.1010 sec/batch\n",
      "Epoch 2/20  Iteration 307/3560 Training loss: 2.2154 0.1980 sec/batch\n",
      "Epoch 2/20  Iteration 308/3560 Training loss: 2.2147 0.0990 sec/batch\n",
      "Epoch 2/20  Iteration 309/3560 Training loss: 2.2139 0.0880 sec/batch\n",
      "Epoch 2/20  Iteration 310/3560 Training loss: 2.2129 0.1140 sec/batch\n",
      "Epoch 2/20  Iteration 311/3560 Training loss: 2.2122 0.1000 sec/batch\n",
      "Epoch 2/20  Iteration 312/3560 Training loss: 2.2116 0.1980 sec/batch\n",
      "Epoch 2/20  Iteration 313/3560 Training loss: 2.2110 0.0930 sec/batch\n",
      "Epoch 2/20  Iteration 314/3560 Training loss: 2.2103 0.1120 sec/batch\n",
      "Epoch 2/20  Iteration 315/3560 Training loss: 2.2097 0.0980 sec/batch\n",
      "Epoch 2/20  Iteration 316/3560 Training loss: 2.2090 0.1000 sec/batch\n",
      "Epoch 2/20  Iteration 317/3560 Training loss: 2.2086 0.1000 sec/batch\n",
      "Epoch 2/20  Iteration 318/3560 Training loss: 2.2078 0.0870 sec/batch\n",
      "Epoch 2/20  Iteration 319/3560 Training loss: 2.2073 0.1130 sec/batch\n",
      "Epoch 2/20  Iteration 320/3560 Training loss: 2.2066 0.1000 sec/batch\n",
      "Epoch 2/20  Iteration 321/3560 Training loss: 2.2059 0.1010 sec/batch\n",
      "Epoch 2/20  Iteration 322/3560 Training loss: 2.2052 0.0880 sec/batch\n",
      "Epoch 2/20  Iteration 323/3560 Training loss: 2.2045 0.1110 sec/batch\n",
      "Epoch 2/20  Iteration 324/3560 Training loss: 2.2040 0.1020 sec/batch\n",
      "Epoch 2/20  Iteration 325/3560 Training loss: 2.2033 0.1010 sec/batch\n",
      "Epoch 2/20  Iteration 326/3560 Training loss: 2.2028 0.1970 sec/batch\n",
      "Epoch 2/20  Iteration 327/3560 Training loss: 2.2021 0.1000 sec/batch\n",
      "Epoch 2/20  Iteration 328/3560 Training loss: 2.2014 0.0990 sec/batch\n",
      "Epoch 2/20  Iteration 329/3560 Training loss: 2.2007 0.0990 sec/batch\n",
      "Epoch 2/20  Iteration 330/3560 Training loss: 2.2003 0.1030 sec/batch\n",
      "Epoch 2/20  Iteration 331/3560 Training loss: 2.1998 0.1010 sec/batch\n",
      "Epoch 2/20  Iteration 332/3560 Training loss: 2.1992 0.0990 sec/batch\n",
      "Epoch 2/20  Iteration 333/3560 Training loss: 2.1985 0.2000 sec/batch\n",
      "Epoch 2/20  Iteration 334/3560 Training loss: 2.1978 0.1010 sec/batch\n",
      "Epoch 2/20  Iteration 335/3560 Training loss: 2.1971 0.1000 sec/batch\n",
      "Epoch 2/20  Iteration 336/3560 Training loss: 2.1963 0.2000 sec/batch\n",
      "Epoch 2/20  Iteration 337/3560 Training loss: 2.1954 0.0880 sec/batch\n",
      "Epoch 2/20  Iteration 338/3560 Training loss: 2.1950 0.1125 sec/batch\n",
      "Epoch 2/20  Iteration 339/3560 Training loss: 2.1944 0.1000 sec/batch\n",
      "Epoch 2/20  Iteration 340/3560 Training loss: 2.1937 0.1010 sec/batch\n",
      "Epoch 2/20  Iteration 341/3560 Training loss: 2.1930 0.1000 sec/batch\n",
      "Epoch 2/20  Iteration 342/3560 Training loss: 2.1924 0.0990 sec/batch\n",
      "Epoch 2/20  Iteration 343/3560 Training loss: 2.1917 0.1000 sec/batch\n",
      "Epoch 2/20  Iteration 344/3560 Training loss: 2.1911 0.1020 sec/batch\n",
      "Epoch 2/20  Iteration 345/3560 Training loss: 2.1905 0.0900 sec/batch\n",
      "Epoch 2/20  Iteration 346/3560 Training loss: 2.1900 0.1080 sec/batch\n",
      "Epoch 2/20  Iteration 347/3560 Training loss: 2.1894 0.1005 sec/batch\n",
      "Epoch 2/20  Iteration 348/3560 Training loss: 2.1887 0.1010 sec/batch\n",
      "Epoch 2/20  Iteration 349/3560 Training loss: 2.1883 0.0990 sec/batch\n",
      "Epoch 2/20  Iteration 350/3560 Training loss: 2.1878 0.1010 sec/batch\n",
      "Epoch 2/20  Iteration 351/3560 Training loss: 2.1875 0.0910 sec/batch\n",
      "Epoch 2/20  Iteration 352/3560 Training loss: 2.1871 0.1100 sec/batch\n",
      "Epoch 2/20  Iteration 353/3560 Training loss: 2.1868 0.0990 sec/batch\n",
      "Epoch 2/20  Iteration 354/3560 Training loss: 2.1862 0.1000 sec/batch\n",
      "Epoch 2/20  Iteration 355/3560 Training loss: 2.1855 0.1020 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20  Iteration 356/3560 Training loss: 2.1849 0.0980 sec/batch\n",
      "Epoch 3/20  Iteration 357/3560 Training loss: 2.1833 0.0990 sec/batch\n",
      "Epoch 3/20  Iteration 358/3560 Training loss: 2.1190 0.1000 sec/batch\n",
      "Epoch 3/20  Iteration 359/3560 Training loss: 2.1067 0.1020 sec/batch\n",
      "Epoch 3/20  Iteration 360/3560 Training loss: 2.0973 0.0990 sec/batch\n",
      "Epoch 3/20  Iteration 361/3560 Training loss: 2.0907 0.1010 sec/batch\n",
      "Epoch 3/20  Iteration 362/3560 Training loss: 2.0824 0.1010 sec/batch\n",
      "Epoch 3/20  Iteration 363/3560 Training loss: 2.0809 0.0890 sec/batch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-b379adfe8bbd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m                     model.initial_state: new_state}\n\u001b[0;32m     21\u001b[0m             batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n\u001b[1;32m---> 22\u001b[1;33m                                                  feed_dict=feed)\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1156\u001b[0m     \u001b[1;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1157\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[1;32m-> 1158\u001b[1;33m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[0;32m   1159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1160\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[0;32m    490\u001b[0m       \u001b[1;31m# Remember the fetch if it is for a tensor handle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m       if (isinstance(fetch, ops.Tensor) and\n\u001b[1;32m--> 492\u001b[1;33m           (fetch.op.type == 'GetSessionHandle' or\n\u001b[0m\u001b[0;32m    493\u001b[0m            fetch.op.type == 'GetSessionHandleV2')):\n\u001b[0;32m    494\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mtype\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2513\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2514\u001b[0m     \u001b[1;34m\"\"\"The type of the op (e.g. `\"MatMul\"`).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2515\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_OperationOpType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2517\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    batch_loss, new_state = sess.run([model.cost, model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}_v{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saved checkpoints\n",
    "\n",
    "Read up on saving and loading checkpoints here: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints\\\\i200_l512_v2.294.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i200_l512_v2.342.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i400_l512_v2.084.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i600_l512_v1.915.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i800_l512_v1.800.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1000_l512_v1.697.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i200_l512_v2.294.ckpt\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints\\i1000_l512_v1.697.ckpt\n",
      "sudher him an the paider and\n",
      "then the she her hem are had been.\n",
      "\n",
      "She wanksed hin ellass and as and house.\n"
     ]
    }
   ],
   "source": [
    "# Change the name of latest checkpoint accordingly\n",
    "checkpoint = \"checkpoints\\\\i1000_l512_v1.697.ckpt\"\n",
    "samp = sample(checkpoint, 100, lstm_size, len(vocab), prime=\"sudh\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
